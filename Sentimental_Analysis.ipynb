{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bharathkumar4281/project/blob/main/Sentimental_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the dataset and do the data preprocessing\n",
        "#problem statement\n",
        "# Develop a sentiment analysis model to classify reviews as positive or negetive.\n",
        "#preprocess the review text using techniques such as lower casting,removing stop words and lemmatization\n",
        "# use the trained model accuratly protect the sentiment\n",
        "import pandas as pd\n",
        "data=pd.read_csv('Reviews.csv')\n",
        "print(data)\n",
        "                                                Review  Liked\n",
        "0                             Wow... Loved this place.      1\n",
        "1                                   Crust is not good.      0\n",
        "2            Not tasty and the texture was just nasty.      0\n",
        "3    Stopped by during the late May bank holiday of...      1\n",
        "4    The selection on the menu was great and so wer...      1\n",
        "..                                                 ...    ...\n",
        "995  I think food should have flavor and texture an...      0\n",
        "996                           Appetite instantly gone.      0\n",
        "997  Overall I was not impressed and would not go b...      0\n",
        "998  The whole experience was underwhelming, and I ...      0\n",
        "999  Then, as if I hadn't wasted enough of my life ...      0\n",
        "\n",
        "[1000 rows x 2 columns]\n",
        "# we can reviw the top5 rows\n",
        "data.head()\n",
        "Review\tLiked\n",
        "0\tWow... Loved this place.\t1\n",
        "1\tCrust is not good.\t0\n",
        "2\tNot tasty and the texture was just nasty.\t0\n",
        "3\tStopped by during the late May bank holiday of...\t1\n",
        "4\tThe selection on the menu was great and so wer...\t1\n",
        "#we can the top 10 rows\n",
        "data.head(10)\n",
        "Review\tLiked\n",
        "0\tWow... Loved this place.\t1\n",
        "1\tCrust is not good.\t0\n",
        "2\tNot tasty and the texture was just nasty.\t0\n",
        "3\tStopped by during the late May bank holiday of...\t1\n",
        "4\tThe selection on the menu was great and so wer...\t1\n",
        "5\tNow I am getting angry and I want my damn pho.\t0\n",
        "6\tHoneslty it didn't taste THAT fresh.)\t0\n",
        "7\tThe potatoes were like rubber and you could te...\t0\n",
        "8\tThe fries were great too.\t1\n",
        "9\tA great touch.\t1\n",
        "data.tail()    # last 5 rows\n",
        "Review\tLiked\n",
        "995\tI think food should have flavor and texture an...\t0\n",
        "996\tAppetite instantly gone.\t0\n",
        "997\tOverall I was not impressed and would not go b...\t0\n",
        "998\tThe whole experience was underwhelming, and I ...\t0\n",
        "999\tThen, as if I hadn't wasted enough of my life ...\t0\n",
        "data.tail(10)   #last 10 rows\n",
        "Review\tLiked\n",
        "990\tThe refried beans that came with my meal were ...\t0\n",
        "991\tSpend your money and time some place else.\t0\n",
        "992\tA lady at the table next to us found a live gr...\t0\n",
        "993\tthe presentation of the food was awful.\t0\n",
        "994\tI can't tell you how disappointed I was.\t0\n",
        "995\tI think food should have flavor and texture an...\t0\n",
        "996\tAppetite instantly gone.\t0\n",
        "997\tOverall I was not impressed and would not go b...\t0\n",
        "998\tThe whole experience was underwhelming, and I ...\t0\n",
        "999\tThen, as if I hadn't wasted enough of my life ...\t0\n",
        "data.info() #dataset info like datatypes,entries of dataset and memory usage\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 1000 entries, 0 to 999\n",
        "Data columns (total 2 columns):\n",
        " #   Column  Non-Null Count  Dtype\n",
        "---  ------  --------------  -----\n",
        " 0   Review  1000 non-null   object\n",
        " 1   Liked   1000 non-null   int64\n",
        "dtypes: int64(1), object(1)\n",
        "memory usage: 15.8+ KB\n",
        "data.isnull().sum() # checking the null values\n",
        "Review    0\n",
        "Liked     0\n",
        "dtype: int64\n",
        "data.duplicated() #checking the duplicated data\n",
        "0      False\n",
        "1      False\n",
        "2      False\n",
        "3      False\n",
        "4      False\n",
        "       ...\n",
        "995    False\n",
        "996    False\n",
        "997    False\n",
        "998    False\n",
        "999    False\n",
        "Length: 1000, dtype: bool\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "pip install wordcloud\n",
        "Requirement already satisfied: wordcloud in c:\\users\\venkatavaibhav\\anaconda3\\lib\\site-packages (1.9.3)\n",
        "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\venkatavaibhav\\anaconda3\\lib\\site-packages (from wordcloud) (1.26.4)\n",
        "Requirement already satisfied: pillow in c:\\users\\venkatavaibhav\\anaconda3\\lib\\site-packages (from wordcloud) (10.2.0)\n",
        "Requirement already satisfied: matplotlib in c:\\users\\venkatavaibhav\\anaconda3\\lib\\site-packages (from wordcloud) (3.8.0)\n",
        "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\venkatavaibhav\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n",
        "Requirement already satisfied: cycler>=0.10 in c:\\users\\venkatavaibhav\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
        "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\venkatavaibhav\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
        "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\venkatavaibhav\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
        "Requirement already satisfied: packaging>=20.0 in c:\\users\\venkatavaibhav\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (23.1)\n",
        "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\venkatavaibhav\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
        "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\venkatavaibhav\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
        "Requirement already satisfied: six>=1.5 in c:\\users\\venkatavaibhav\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
        "Note: you may need to restart the kernel to use updated packages.\n",
        "from wordcloud import WordCloud\n",
        "combined_text=\" \".join(data['Review']) #Combine all review text into one string\n",
        "wordcloud=WordCloud(width=800,height=400,background_color='white').generate(combined_text)\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.imshow(wordcloud,interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Reviews')\n",
        "plt.show()\n",
        "\n",
        "from collections import Counter\n",
        "targeted_words=['good','great','amazing','bad']\n",
        "all_words=\" \".join(data['Review']).lower().split()     #flattened reviews into a single list of words\n",
        "word_counts=Counter(all_words)\n",
        "target_word_count={word:word_counts[word] for word in targeted_words}\n",
        "#plotting\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.bar(target_word_count.keys(),target_word_count.values(),color=['blue','green','orange','red'])\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Frequency of specific words in reviews')\n",
        "plt.show()\n",
        "\n",
        "lowercases_text=data['Review'].str.lower()\n",
        "print(lowercases_text)\n",
        "0                               wow... loved this place.\n",
        "1                                     crust is not good.\n",
        "2              not tasty and the texture was just nasty.\n",
        "3      stopped by during the late may bank holiday of...\n",
        "4      the selection on the menu was great and so wer...\n",
        "                             ...\n",
        "995    i think food should have flavor and texture an...\n",
        "996                             appetite instantly gone.\n",
        "997    overall i was not impressed and would not go b...\n",
        "998    the whole experience was underwhelming, and i ...\n",
        "999    then, as if i hadn't wasted enough of my life ...\n",
        "Name: Review, Length: 1000, dtype: object\n",
        "#tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "#Download the'punkt' tokenizer\n",
        "nltk.download('punkt')\n",
        "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
        "[nltk_data]     getaddrinfo failed>\n",
        "False\n",
        "data['Tokens'] = data['Review'].apply(word_tokenize)\n",
        "print(data ['Tokens'] )\n",
        "0                      [Wow, ..., Loved, this, place, .]\n",
        "1                              [Crust, is, not, good, .]\n",
        "2      [Not, tasty, and, the, texture, was, just, nas...\n",
        "3      [Stopped, by, during, the, late, May, bank, ho...\n",
        "4      [The, selection, on, the, menu, was, great, an...\n",
        "                             ...\n",
        "995    [I, think, food, should, have, flavor, and, te...\n",
        "996                       [Appetite, instantly, gone, .]\n",
        "997    [Overall, I, was, not, impressed, and, would, ...\n",
        "998    [The, whole, experience, was, underwhelming, ,...\n",
        "999    [Then, ,, as, if, I, had, n't, wasted, enough,...\n",
        "Name: Tokens, Length: 1000, dtype: object\n",
        "data.info()\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 1000 entries, 0 to 999\n",
        "Data columns (total 3 columns):\n",
        " #   Column  Non-Null Count  Dtype\n",
        "---  ------  --------------  -----\n",
        " 0   Review  1000 non-null   object\n",
        " 1   Liked   1000 non-null   int64\n",
        " 2   Tokens  1000 non-null   object\n",
        "dtypes: int64(1), object(2)\n",
        "memory usage: 23.6+ KB\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "[nltk_data] Downloading package stopwords to\n",
        "[nltk_data]     C:\\Users\\Venkatavaibhav\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
        "True\n",
        "stop_words=set(stopwords.words('english'))\n",
        "data['Tokens']=data['Review'].apply(lambda x:[word for word in word_tokenize(x) if word not in stop_words])\n",
        "print(data['Tokens'])\n",
        "0                            [Wow, ..., Loved, place, .]\n",
        "1                                       [Crust, good, .]\n",
        "2                        [Not, tasty, texture, nasty, .]\n",
        "3      [Stopped, late, May, bank, holiday, Rick, Stev...\n",
        "4               [The, selection, menu, great, prices, .]\n",
        "                             ...\n",
        "995        [I, think, food, flavor, texture, lacking, .]\n",
        "996                       [Appetite, instantly, gone, .]\n",
        "997          [Overall, I, impressed, would, go, back, .]\n",
        "998    [The, whole, experience, underwhelming, ,, I, ...\n",
        "999    [Then, ,, I, n't, wasted, enough, life, ,, pou...\n",
        "Name: Tokens, Length: 1000, dtype: object\n",
        "#stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "Stemmer=PorterStemmer()\n",
        "data['stemmed']=data['Review'].apply(lambda x:\"\".join([Stemmer.stem(word) for word in word_tokenize(x)]))\n",
        "print(data['stemmed'])\n",
        "data['stemmed'].value_counts()\n",
        "0                                    wow...lovethiplace.\n",
        "1                                        crustisnotgood.\n",
        "2                       nottastiandthetexturwajustnasti.\n",
        "3      stopbydurethelatemaybankholidayoffricksteverec...\n",
        "4            theselectonthemenuwagreatandsoweretheprice.\n",
        "                             ...\n",
        "995    ithinkfoodshouldhaveflavorandtexturandbothwere...\n",
        "996                                appetitinstantligone.\n",
        "997                overaliwanotimpressandwouldnotgoback.\n",
        "998    thewholeexperiwaunderwhelm,andithinkwe'lljustg...\n",
        "999    then,asifihadn'twastenoughofmylifethere,theypo...\n",
        "Name: stemmed, Length: 1000, dtype: object\n",
        "stemmed\n",
        "#name?                                                                                                  4\n",
        "thefoodwaterribl.                                                                                       2\n",
        "iwouldnotrecommendthiplace.                                                                             2\n",
        "ilovethiplace.                                                                                          2\n",
        "iwon'tbeback.                                                                                           2\n",
        "                                                                                                       ..\n",
        "omg,thefoodwadelicioso!                                                                                 1\n",
        "thereisnothauthentaboutthiplace.                                                                        1\n",
        "thespaghettiisnothspecialwhatsoev.                                                                      1\n",
        "ofallthedish,thesalmonwathebest,butallweregreat.                                                        1\n",
        "then,asifihadn'twastenoughofmylifethere,theypoursaltinthewoundbydrawoutthetimeittooktobringthecheck.    1\n",
        "Name: count, Length: 993, dtype: int64\n",
        "#Lemmatization\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "[nltk_data] Downloading package wordnet to\n",
        "[nltk_data]     C:\\Users\\Venkatavaibhav\\AppData\\Roaming\\nltk_data...\n",
        "[nltk_data]   Package wordnet is already up-to-date!\n",
        "True\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "data['Lemmatized']=data['Review'].apply(lambda x:''.join([lemmatizer.lemmatize(word,pos=wordnet.VERB)for word in word_tokenize(x)]))\n",
        "print(data['Lemmatized'])\n",
        "0                                  Wow...Lovedthisplace.\n",
        "1                                        Crustbenotgood.\n",
        "2                      Nottastyandthetexturebejustnasty.\n",
        "3      StoppedbyduringthelateMaybankholidayoffRickSte...\n",
        "4           Theselectiononthemenubegreatandsobetheprice.\n",
        "                             ...\n",
        "995    Ithinkfoodshouldhaveflavorandtextureandbothbel...\n",
        "996                                 Appetiteinstantlygo.\n",
        "997               OverallIbenotimpressandwouldnotgoback.\n",
        "998    Thewholeexperiencebeunderwhelming,andIthinkwe'...\n",
        "999    Then,asifIhaven'twasteenoughofmylifethere,they...\n",
        "Name: Lemmatized, Length: 1000, dtype: object\n",
        "#removing the numbers\n",
        "import re\n",
        "data['No_Numbers']=data['Review'].apply(lambda x:re.sub(r'\\d+','',x))\n",
        "print(data['No_Numbers'])\n",
        "0                               Wow... Loved this place.\n",
        "1                                     Crust is not good.\n",
        "2              Not tasty and the texture was just nasty.\n",
        "3      Stopped by during the late May bank holiday of...\n",
        "4      The selection on the menu was great and so wer...\n",
        "                             ...\n",
        "995    I think food should have flavor and texture an...\n",
        "996                             Appetite instantly gone.\n",
        "997    Overall I was not impressed and would not go b...\n",
        "998    The whole experience was underwhelming, and I ...\n",
        "999    Then, as if I hadn't wasted enough of my life ...\n",
        "Name: No_Numbers, Length: 1000, dtype: object\n",
        "data[\"cleaned_text\"]=data['Review'].apply(lambda x:re.sub(r'[^A-Za-z0-9\\s]','',x))\n",
        "print(data[\"cleaned_text\"])\n",
        "0                                   Wow Loved this place\n",
        "1                                      Crust is not good\n",
        "2               Not tasty and the texture was just nasty\n",
        "3      Stopped by during the late May bank holiday of...\n",
        "4      The selection on the menu was great and so wer...\n",
        "                             ...\n",
        "995    I think food should have flavor and texture an...\n",
        "996                              Appetite instantly gone\n",
        "997    Overall I was not impressed and would not go back\n",
        "998    The whole experience was underwhelming and I t...\n",
        "999    Then as if I hadnt wasted enough of my life th...\n",
        "Name: cleaned_text, Length: 1000, dtype: object\n",
        "data.info()\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "RangeIndex: 1000 entries, 0 to 999\n",
        "Data columns (total 7 columns):\n",
        " #   Column        Non-Null Count  Dtype\n",
        "---  ------        --------------  -----\n",
        " 0   Review        1000 non-null   object\n",
        " 1   Liked         1000 non-null   int64\n",
        " 2   Tokens        1000 non-null   object\n",
        " 3   stemmed       1000 non-null   object\n",
        " 4   Lemmatized    1000 non-null   object\n",
        " 5   No_Numbers    1000 non-null   object\n",
        " 6   cleaned_text  1000 non-null   object\n",
        "dtypes: int64(1), object(6)\n",
        "memory usage: 54.8+ KB\n",
        "pip install contractions\n",
        "Requirement already satisfied: contractions in c:\\users\\venkatavaibhav\\anaconda3\\lib\\site-packages (0.1.73)\n",
        "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\venkatavaibhav\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
        "Requirement already satisfied: anyascii in c:\\users\\venkatavaibhav\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
        "Requirement already satisfied: pyahocorasick in c:\\users\\venkatavaibhav\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
        "Note: you may need to restart the kernel to use updated packages.\n",
        "#normalization\n",
        "import contractions\n",
        "data['Expanded'] = data['Review'].apply(contractions.fix)\n",
        "print(data['Expanded'])\n",
        "0                               Wow... Loved this place.\n",
        "1                                     Crust is not good.\n",
        "2              Not tasty and the texture was just nasty.\n",
        "3      Stopped by during the late May bank holiday of...\n",
        "4      The selection on the menu was great and so wer...\n",
        "                             ...\n",
        "995    I think food should have flavor and texture an...\n",
        "996                             Appetite instantly gone.\n",
        "997    Overall I was not impressed and would not go b...\n",
        "998    The whole experience was underwhelming, and I ...\n",
        "999    Then, as if I had not wasted enough of my life...\n",
        "Name: Expanded, Length: 1000, dtype: object\n",
        "pip install emoji\n",
        "Collecting emoji\n",
        "  Downloading emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
        "Requirement already satisfied: typing-extensions>=4.7.0 in c:\\users\\venkatavaibhav\\anaconda3\\lib\\site-packages (from emoji) (4.9.0)\n",
        "Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
        "   ---------------------------------------- 0.0/431.4 kB ? eta -:--:--\n",
        "    --------------------------------------- 10.2/431.4 kB ? eta -:--:--\n",
        "   -- ------------------------------------ 30.7/431.4 kB 435.7 kB/s eta 0:00:01\n",
        "   ----- --------------------------------- 61.4/431.4 kB 656.4 kB/s eta 0:00:01\n",
        "   ------------------ --------------------- 204.8/431.4 kB 1.6 MB/s eta 0:00:01\n",
        "   ---------------------------------------  430.1/431.4 kB 2.4 MB/s eta 0:00:01\n",
        "   ---------------------------------------- 431.4/431.4 kB 2.3 MB/s eta 0:00:00\n",
        "Installing collected packages: emoji\n",
        "Successfully installed emoji-2.12.1\n",
        "Note: you may need to restart the kernel to use updated packages.\n",
        "import emoji\n",
        "data['Emoji'] = data['Review'].apply(emoji.demojize)\n",
        "print(data['Emoji'])\n",
        "0                               Wow... Loved this place.\n",
        "1                                     Crust is not good.\n",
        "2              Not tasty and the texture was just nasty.\n",
        "3      Stopped by during the late May bank holiday of...\n",
        "4      The selection on the menu was great and so wer...\n",
        "                             ...\n",
        "995    I think food should have flavor and texture an...\n",
        "996                             Appetite instantly gone.\n",
        "997    Overall I was not impressed and would not go b...\n",
        "998    The whole experience was underwhelming, and I ...\n",
        "999    Then, as if I hadn't wasted enough of my life ...\n",
        "Name: Emoji, Length: 1000, dtype: object\n",
        "pip install beautifulsoup4\n",
        "Requirement already satisfied: beautifulsoup4 in c:\\users\\venkatavaibhav\\anaconda3\\lib\\site-packages (4.12.2)\n",
        "Requirement already satisfied: soupsieve>1.2 in c:\\users\\venkatavaibhav\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
        "Note: you may need to restart the kernel to use updated packages.\n",
        "from bs4 import BeautifulSoup\n",
        "data['Cleaned'] = data['Review'].apply(lambda x: BeautifulSoup(x,\"html.parser\").get_text())\n",
        "print(data['Cleaned'])\n",
        "0                               Wow... Loved this place.\n",
        "1                                     Crust is not good.\n",
        "2              Not tasty and the texture was just nasty.\n",
        "3      Stopped by during the late May bank holiday of...\n",
        "4      The selection on the menu was great and so wer...\n",
        "                             ...\n",
        "995    I think food should have flavor and texture an...\n",
        "996                             Appetite instantly gone.\n",
        "997    Overall I was not impressed and would not go b...\n",
        "998    The whole experience was underwhelming, and I ...\n",
        "999    Then, as if I hadn't wasted enough of my life ...\n",
        "Name: Cleaned, Length: 1000, dtype: object\n",
        "C:\\Users\\Venkatavaibhav\\AppData\\Local\\Temp\\ipykernel_10064\\4263549124.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
        "  data['Cleaned'] = data['Review'].apply(lambda x: BeautifulSoup(x,\"html.parser\").get_text())\n",
        "#TF -IDF VECTORIZER\n",
        "#TF = TERM FREQUENCY,IDF = INVERSE DOCUMENT FREQUENCY\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer=TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(data['Review'])\n",
        "print(X.toarray())\n",
        "[[0. 0. 0. ... 0. 0. 0.]\n",
        " [0. 0. 0. ... 0. 0. 0.]\n",
        " [0. 0. 0. ... 0. 0. 0.]\n",
        " ...\n",
        " [0. 0. 0. ... 0. 0. 0.]\n",
        " [0. 0. 0. ... 0. 0. 0.]\n",
        " [0. 0. 0. ... 0. 0. 0.]]\n",
        "#Building the machine learning model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score,classification_report\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(data['Review'])\n",
        "y = data['Liked']\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state = 42)\n",
        "print(X_train,X_test,y_train,y_test)\n",
        "  (0, 1527)\t0.578582388550776\n",
        "  (0, 1516)\t0.5587881819736206\n",
        "  (0, 2005)\t0.45560954528848707\n",
        "  (0, 1938)\t0.20794801889277578\n",
        "  (0, 1778)\t0.3196494189359375\n",
        "  (1, 711)\t0.4193390530637025\n",
        "  (1, 622)\t0.4193390530637025\n",
        "  (1, 1180)\t0.3554925882290983\n",
        "  (1, 613)\t0.32026666852400937\n",
        "  (1, 1471)\t0.272554926639921\n",
        "  (1, 62)\t0.5164170315313602\n",
        "  (1, 263)\t0.27998403419950374\n",
        "  (2, 1057)\t0.5102287539842273\n",
        "  (2, 1962)\t0.48790050997574835\n",
        "  (2, 699)\t0.3031291394604372\n",
        "  (2, 1618)\t0.5102287539842273\n",
        "  (2, 1571)\t0.3198299691276987\n",
        "  (2, 1938)\t0.21704341786509082\n",
        "  (3, 1723)\t0.4538927163856403\n",
        "  (3, 1852)\t0.42838721919777234\n",
        "  (3, 719)\t0.30305480231081033\n",
        "  (3, 1791)\t0.3192733609259723\n",
        "  (3, 821)\t0.37508854376741335\n",
        "  (3, 590)\t0.33614831815132157\n",
        "  (3, 1471)\t0.29501353406759745\n",
        "  :\t:\n",
        "  (797, 1796)\t0.2155139089210878\n",
        "  (798, 649)\t0.34296597014320795\n",
        "  (798, 787)\t0.34296597014320795\n",
        "  (798, 17)\t0.34296597014320795\n",
        "  (798, 406)\t0.32369375609078\n",
        "  (798, 18)\t0.34296597014320795\n",
        "  (798, 122)\t0.34296597014320795\n",
        "  (798, 1320)\t0.29941363785664343\n",
        "  (798, 890)\t0.2907476885588123\n",
        "  (798, 1206)\t0.14757149730549846\n",
        "  (798, 1776)\t0.1774987010047913\n",
        "  (798, 944)\t0.14171411086951774\n",
        "  (798, 1938)\t0.2228482091437908\n",
        "  (798, 63)\t0.09941818111459709\n",
        "  (799, 158)\t0.3964954268406326\n",
        "  (799, 279)\t0.4286481233974752\n",
        "  (799, 1371)\t0.36691217304779694\n",
        "  (799, 1835)\t0.36691217304779694\n",
        "  (799, 179)\t0.2837174144687825\n",
        "  (799, 245)\t0.341391142036711\n",
        "  (799, 699)\t0.20607571915436035\n",
        "  (799, 915)\t0.20607571915436035\n",
        "  (799, 2025)\t0.24443151499428203\n",
        "  (799, 944)\t0.1876634520751288\n",
        "  (799, 1778)\t0.11340570263380041   (0, 762)\t0.47256008939307\n",
        "  (0, 830)\t0.5227794349477328\n",
        "  (0, 850)\t0.2788148086554363\n",
        "  (0, 903)\t0.32903415421009913\n",
        "  (0, 756)\t0.29720151212237056\n",
        "  (0, 2025)\t0.28135721691301263\n",
        "  (0, 1197)\t0.387165119592869\n",
        "  (1, 1658)\t0.4103419207551196\n",
        "  (1, 45)\t0.4103419207551196\n",
        "  (1, 1781)\t0.29575708998809846\n",
        "  (1, 621)\t0.2766223654390893\n",
        "  (1, 1851)\t0.32480705768080226\n",
        "  (1, 1633)\t0.27939699659872946\n",
        "  (1, 716)\t0.26026227204972024\n",
        "  (1, 696)\t0.17703009835601147\n",
        "  (1, 915)\t0.18618945297796038\n",
        "  (1, 1812)\t0.15350378435384465\n",
        "  (1, 1571)\t0.19644751772739483\n",
        "  (1, 1778)\t0.1024620747393945\n",
        "  (1, 63)\t0.11894896563501532\n",
        "  (1, 1759)\t0.3084469642914332\n",
        "  (2, 641)\t0.36530419498595135\n",
        "  (2, 339)\t0.35132051079261234\n",
        "  (2, 654)\t0.3398950088821647\n",
        "  (2, 1471)\t0.5877990721015991\n",
        "  :\t:\n",
        "  (198, 73)\t0.39191145656618687\n",
        "  (198, 719)\t0.30866733735581975\n",
        "  (198, 1641)\t0.35918322904387284\n",
        "  (198, 1982)\t0.27535929343010807\n",
        "  (198, 850)\t0.24655854617575115\n",
        "  (198, 155)\t0.2373165782739377\n",
        "  (198, 696)\t0.19944536250717973\n",
        "  (198, 745)\t0.3659336009279139\n",
        "  (198, 1194)\t0.2043973840668572\n",
        "  (199, 245)\t0.3138625483855926\n",
        "  (199, 1057)\t0.31889764742751303\n",
        "  (199, 1894)\t0.27621491029163275\n",
        "  (199, 1815)\t0.3645234154260868\n",
        "  (199, 1552)\t0.3774360932885095\n",
        "  (199, 1780)\t0.2608394295039084\n",
        "  (199, 1370)\t0.284302533387564\n",
        "  (199, 1812)\t0.15619894024506778\n",
        "  (199, 1807)\t0.24136874538352923\n",
        "  (199, 166)\t0.27621491029163275\n",
        "  (199, 944)\t0.17253092436945436\n",
        "  (199, 1938)\t0.135654125343441\n",
        "  (199, 1778)\t0.10426106142577002\n",
        "  (199, 63)\t0.12103742232574498\n",
        "  (199, 763)\t0.19662646224587704\n",
        "  (199, 942)\t0.16732136895679153 29     0\n",
        "535    1\n",
        "695    0\n",
        "557    0\n",
        "836    1\n",
        "      ..\n",
        "106    1\n",
        "270    1\n",
        "860    1\n",
        "435    0\n",
        "102    1\n",
        "Name: Liked, Length: 800, dtype: int64 521    1\n",
        "737    1\n",
        "740    1\n",
        "660    1\n",
        "411    1\n",
        "      ..\n",
        "408    1\n",
        "332    1\n",
        "208    0\n",
        "613    0\n",
        "78     1\n",
        "Name: Liked, Length: 200, dtype: int64\n",
        "model=MultinomialNB()\n",
        "model.fit(X_train,y_train)\n",
        "MultinomialNB()\n",
        "In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.\n",
        "On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n",
        "y_pred=model.predict(X_test)\n",
        "accuracy= accuracy_score(y_test,y_pred)\n",
        "report=classification_report(y_test,y_pred)\n",
        "print(f'Accuracy:{accuracy}')\n",
        "print('Classification Report')\n",
        "print(report)\n",
        "Accuracy:0.8\n",
        "Classification Report\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.76      0.85      0.80        96\n",
        "           1       0.85      0.75      0.80       104\n",
        "\n",
        "    accuracy                           0.80       200\n",
        "   macro avg       0.80      0.80      0.80       200\n",
        "weighted avg       0.81      0.80      0.80       200\n",
        "\n",
        "#prediction sentiment fro new review\n",
        "def predict_sentiment(new_review):\n",
        "    # Assuming you have a preprocess_text function defined elsewhere\n",
        "    cleaned_review=preprocess_text(new_review) # Apply preprocessing to the new review\n",
        "    X_new = vectorizer.transform([cleaned_review]) # Transform the cleaned review using the fitted vectorizer\n",
        "    return model.predict(X_new)[0] # Predict the sentiment using the trained model\n",
        "\n",
        "# Get new reviews from user input\n",
        "new_reviews= input(\"enter a review:\")\n",
        "# Iterate through each review in the input\n",
        "for review in new_reviews:\n",
        "    sentiment = predict_sentiment(review) # Predict the sentiment of the current review\n",
        "    sentiment_label='Positive' if sentiment ==1 else \"Negative\" # Assign sentiment label based on prediction\n",
        "    print(f\"Review:'{review}' \\nSentiment: {sentiment_label}\") # Print the review and its predicted sentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "6Q4KmR-HP7CT",
        "outputId": "579a423d-5d70-4cb6-f25d-1c95829cfe68"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-3-7d70c706d5e9>, line 9)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-7d70c706d5e9>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    Review  Liked\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRK4pHkfWG55nxhc3x9WKI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}